## ðŸ“Š Metrics & Evaluation Notes

This project is intended as a **personal RAG / portfolio system**, so all metrics should be interpreted in that context rather than as hard production SLAs.

### 1. Runtime Metrics (built into the API)

The FastAPI backend exposes a `/metrics` endpoint and feeds data into the Streamlit Analytics tab. Out of the box it tracks:

- **`requests_total`** â€“ total HTTP requests handled by the API
- **`questions_total`** â€“ number of Q&A requests
- **`feedback_total`** â€“ how many feedback events were submitted
- **`errors_total`** â€“ number of failed requests (status code â‰¥ 400)
- **`average_latency_ms`** â€“ mean latency since startup
- **`uptime_seconds`** â€“ how long the API has been running
- **`error_rate_percent`** â€“ error percentage over all requests

These numbers are **environment-dependent** (your laptop vs. a cloud VM) and are not pinned to any fixed target like â€œ<500 ms p95â€.

### 2. Retrieval / RAG Quality Metrics

The codebase is structured so that you can plug in your own evaluation scripts for:

- **Recall@K**
- **MRR / NDCG**
- **Simple hit-rate style checks for specific questions**

Typical workflow:

1. Prepare a small evaluation set of questionâ€“answer or questionâ€“document pairs that reflect your personal knowledge base.
2. Run them through the `/ask` endpoint or a small Python script using the same retriever.
3. Compute Recall@K / MRR / NDCG using your favourite library or a custom script.

Because this is a personal portfolio project, **no global â€œ95%+ Recall@5â€ claim is hard-coded here**. Any numbers you publish should clearly state:

- the dataset (e.g., â€œ50 manually curated interview-style questions about my CVâ€)
- the hardware (e.g., â€œlocal M1 MacBook Proâ€)
- the configuration (mock vs. OpenAI mode, K value, temperature, verification on/off)

### 3. How to Report Numbers in a Resume or README

To keep things honest and reproducible, prefer wording like:

- â€œImplemented evaluation hooks (Recall@K/MRR) and a small benchmark script; results depend on dataset and hardware.â€
- â€œOn my local test set of 50 career questions, the RAG pipeline correctly surfaced a relevant document in the topâ€‘3 ~X% of the time.â€
- â€œLatency on my laptop is typically in the **Yâ€“Z ms** range for mock mode; OpenAI mode adds network overhead.â€

Avoid claiming universal guarantees (for example â€œ99.9% uptimeâ€ or â€œ95%+ recallâ€) unless you have longâ€‘running logs and a public script that others can run to reproduce them.


