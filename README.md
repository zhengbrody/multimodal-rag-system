# ğŸ’¬ Personal RAG Q&A System

> **Intelligent Q&A system for personal websites powered by RAG technology with advanced anti-hallucination strategies**

[![Python 3.11-3.12](https://img.shields.io/badge/python-3.11--3.12-blue.svg)](https://www.python.org/downloads/)
[![FastAPI](https://img.shields.io/badge/FastAPI-0.104+-green.svg)](https://fastapi.tiangolo.com/)
[![Streamlit](https://img.shields.io/badge/Streamlit-1.28+-red.svg)](https://streamlit.io/)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)

## ğŸ”— Quick Links

- **Live Demo / Frontend**: `http://localhost:8501` (after running `streamlit run frontend/personal_app.py`)
- **API Docs (FastAPI)**: `http://localhost:8000/docs`
- **Quickstart**: [Getting Started](#-quick-start)
- **Architecture & Metrics**: [Architecture](#-architecture) Â· [Metrics & Evaluation](docs/metrics.md)

## ğŸ¯ Overview

This project demonstrates a **production-ready RAG (Retrieval-Augmented Generation) system** specifically designed for personal website Q&A functionality. Visitors can ask questions about you in natural language, and the system provides accurate, context-aware answers based on your personal knowledge base.

### Key Highlights

- âœ… **Anti-Hallucination Strategies**: Multiple mechanisms ensure AI doesn't fabricate information
- âœ… **Production-Ready**: Structured logging, metrics, monitoring, and feedback collection
- âœ… **Dual Mode**: Mock mode (no API costs) and OpenAI mode (high-quality embeddings)
- âœ… **Modern UI**: Conversation-style interface with real-time feedback
- âœ… **Comprehensive Testing**: Unit tests, integration tests, and CI/CD pipeline

## ğŸ—ï¸ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Streamlit UI   â”‚  (Modern Conversation Interface)
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   FastAPI API   â”‚  (REST Endpoints + Metrics)
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
    â”Œâ”€â”€â”€â”€â”´â”€â”€â”€â”€â”
    â–¼         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Mock   â”‚ â”‚  OpenAI  â”‚  (Embedding Models)
â”‚ Retrieverâ”‚ â”‚ Embeddingsâ”‚
â””â”€â”€â”€â”¬â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜
    â”‚           â”‚
    â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜
          â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚ Knowledgeâ”‚  (Personal Data)
    â”‚   Base   â”‚
    â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜
          â”‚
          â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚   LLM    â”‚  (GPT-3.5/4)
    â”‚ Reasoningâ”‚  (Low Temp + Strict Prompts)
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
          â”‚
          â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚ Response â”‚  (With Confidence + Sources)
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ›¡ï¸ Anti-Hallucination Strategies

This system implements **4 key strategies** to prevent AI fabrication:

### 1. **Low Temperature Generation** (0.3)
- Reduces randomness in LLM responses
- Increases determinism and factual accuracy
- Configurable via environment variables

### 2. **Strict Prompt Engineering**
- Explicit system prompts: "Only use provided context"
- Clear instructions to state when information is unavailable
- No speculation or fabrication allowed

### 3. **Confidence Assessment**
- Three-level confidence scoring (High/Medium/Low)
- Based on retrieval similarity scores
- Visual indicators in UI

### 4. **Source Tracing & Verification**
- Every answer includes source documents
- Relevance scores for each source
- Optional second-pass verification mode

## âœ¨ Features

### Core Capabilities
- ğŸ”¤ **Semantic Search** - Natural language queries with OpenAI embeddings
- ğŸ¤– **RAG-Powered Q&A** - Contextual answers with LLM integration
- ğŸ’¬ **Conversation Mode** - Multi-turn dialogue with context retention
- ğŸ“Š **Real-time Metrics** - Monitor system performance and latency
- ğŸ’­ **User Feedback** - Collect ratings to improve quality

### Technical Features
- **Dual Retrieval Modes**: Mock (keyword-based, no API) or OpenAI (semantic embeddings)
- **Structured Logging**: JSON-formatted logs for production monitoring
- **Metrics Endpoint**: Prometheus-compatible metrics for observability
- **Feedback Collection**: Store user feedback for continuous improvement
- **Category Weighting**: Boost important document categories (FAQ, personal info)

## ğŸš€ Quick Start

### Prerequisites
- Python 3.11-3.12
- OpenAI API key (optional, for OpenAI mode)

### Installation

1. **Clone the repository**
```bash
git clone https://github.com/yourusername/multimodal-rag-system.git
cd multimodal-rag-system
```

2. **Create virtual environment**
```bash
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate
```

3. **Install dependencies**
```bash
# Install lightweight dependencies (recommended)
pip install -r requirements_simple.txt
```

4. **Configure environment**
```bash
cp .env.example .env
# Edit .env and add your OPENAI_API_KEY (optional for mock mode)
```

5. **Prepare knowledge base**
```bash
# Edit data/raw/knowledge_base.json with your personal information
# See docs/PERSONAL_RAG_README.md for structure details
```

6. **Start the system**
```bash
# Option 1: Use the run script (starts both API and frontend)
python run.py

# Option 2: Start separately
# Terminal 1: API
uvicorn src.api.personal_api:app --reload --port 8000

# Terminal 2: Frontend
streamlit run frontend/personal_app.py
```

8. **Access the application**
   - Frontend: http://localhost:8501
   - API Docs: http://localhost:8000/docs
   - Health Check: http://localhost:8000/health

## ğŸ“– Usage

### API Endpoints

#### Ask a Question
```bash
curl -X POST "http://localhost:8000/ask" \
  -H "Content-Type: application/json" \
  -d '{
    "question": "What technologies are you proficient in?",
    "k": 5,
    "use_verification": false,
    "conversational": false
  }'
```

#### Get Metrics
```bash
curl http://localhost:8000/metrics
```

#### Submit Feedback
```bash
curl -X POST "http://localhost:8000/feedback" \
  -H "Content-Type: application/json" \
  -d '{
    "question": "What is your experience?",
    "answer": "...",
    "rating": 5,
    "helpful": true
  }'
```

### Frontend Features

- **ğŸ’¬ Chat Interface**: Conversation-style UI with message bubbles
- **ğŸ“Š Analytics Tab**: View system metrics and performance
- **ğŸŒ“ Theme Toggle**: Light/dark mode support
- **ğŸ’­ Feedback**: Rate answers with emoji reactions
- **ğŸ“– Source Viewing**: Expand to see where answers come from

## ğŸ§ª Testing

Run the test suite:

```bash
# Install test dependencies
pip install pytest pytest-cov

# Run all tests
pytest tests/ -v

# Run with coverage
pytest tests/ --cov=src --cov-report=html

# Run specific test file
pytest tests/test_api.py -v
```

## ğŸ“ Project Structure

```
multimodal-rag-system/
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/
â”‚   â”‚   â””â”€â”€ knowledge_base.json    # Your personal data
â”‚   â””â”€â”€ processed/
â”‚       â”œâ”€â”€ retriever.pkl          # OpenAI embeddings index
â”‚       â””â”€â”€ mock_retriever.pkl     # Mock keyword index
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ api/
â”‚   â”‚   â””â”€â”€ personal_api.py        # FastAPI backend
â”‚   â”œâ”€â”€ rag/
â”‚   â”‚   â”œâ”€â”€ knowledge_processor.py # Knowledge base builder
â”‚   â”‚   â”œâ”€â”€ retriever.py           # OpenAI retriever
â”‚   â”‚   â”œâ”€â”€ mock_retriever.py      # Mock retriever
â”‚   â”‚   â””â”€â”€ pipeline.py            # RAG pipeline with anti-hallucination
â”‚   â””â”€â”€ utils/
â”‚       â”œâ”€â”€ config.py              # Configuration management
â”‚       â””â”€â”€ logger.py              # Structured logging
â”œâ”€â”€ frontend/
â”‚   â””â”€â”€ personal_app.py            # Streamlit UI
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ test_api.py                # API tests
â”‚   â””â”€â”€ test_retriever.py          # Retriever tests
â”œâ”€â”€ docs/                          # Additional documentation
â”‚   â”œâ”€â”€ PERSONAL_RAG_README.md     # Detailed personal RAG doc
â”‚   â”œâ”€â”€ DEPLOYMENT.md              # Deployment guide
â”‚   â”œâ”€â”€ WORKFLOW.md                # Recommended workflow
â”‚   â”œâ”€â”€ TROUBLESHOOTING.md         # General troubleshooting
â”‚   â””â”€â”€ TERMINAL_TROUBLESHOOTING.md# Terminal-specific tips
â”œâ”€â”€ scripts/                       # Helper scripts
â”‚   â”œâ”€â”€ run_simple.sh              # One-click local start
â”‚   â”œâ”€â”€ start_api.sh               # Start API only
â”‚   â”œâ”€â”€ restart.sh                 # Restart services
â”‚   â”œâ”€â”€ rebuild_index.sh           # Rebuild knowledge index
â”‚   â””â”€â”€ quick_fix.sh               # Common terminal fixes
â”œâ”€â”€ .env.example                   # Environment template
â”œâ”€â”€ requirements_simple.txt        # Lightweight dependencies
â”œâ”€â”€ requirements.txt               # Full dependencies (optional)
â”œâ”€â”€ run.py                         # Launch script (API + UI)
â””â”€â”€ README.md                      # This file
```

## ğŸ”§ Configuration

### Environment Variables

```bash
# Required for OpenAI mode
OPENAI_API_KEY=sk-...

# Optional
LLM_MODEL=gpt-3.5-turbo          # or gpt-4
USE_MOCK=true                     # Use mock mode (no API costs)
API_URL=http://localhost:8000
LOG_LEVEL=INFO
```

### Knowledge Base Structure

See `docs/PERSONAL_RAG_README.md` for detailed structure. Key sections:
- `personal_info`: Basic information
- `skills`: Technical skills
- `projects`: Project experience
- `experience`: Work history
- `education`: Education background
- `faq`: Frequently asked questions

## ğŸ“Š Performance Metrics

The system tracks:
- **Request Count**: Total API requests
- **Average Latency**: Response time in milliseconds
- **Error Rate**: Percentage of failed requests
- **Question Count**: Total questions answered
- **Feedback Count**: User feedback submissions

Access via `/metrics` endpoint or frontend Analytics tab.

## ğŸš¢ Deployment

### Docker Deployment

```bash
# Build and run
docker-compose up --build

# Access services
# Frontend: http://localhost:8501
# API: http://localhost:8000
```

### Streamlit Cloud

1. Push code to GitHub
2. Connect to Streamlit Cloud
3. Set main file: `frontend/personal_app.py`
4. Add secrets: `API_URL` (your backend URL)

See `docs/DEPLOYMENT.md` for detailed instructions.

## ğŸ’¼ Resume Highlights

This project demonstrates:

### Technical Skills
- **Machine Learning**: RAG pipeline design, embedding models, semantic search
- **Backend Development**: FastAPI, REST APIs, structured logging, metrics
- **Frontend Development**: Streamlit, modern UI/UX design
- **MLOps**: Model deployment, monitoring, feedback loops
- **Software Engineering**: Testing, CI/CD, code quality

### Key Achievements
- âœ… Implemented 4 anti-hallucination strategies (prompt constraints, low temperature, confidence scoring, source tracing)
- âœ… Added metrics and logging hooks so latency, error rate and request volume can be measured per environment
- âœ… Created modern conversation UI with real-time feedback and analytics
- âœ… Set up a testable API and project structure; coverage and performance depend on the dataset and hardware you run it on
- âœ… Designed dual-mode system (mock/OpenAI) so you can iterate UI and tests without incurring API costs

## ğŸ§  Design Decisions (Why these choices?)

- **Why mock mode?**  
  Added a mock retriever and mock RAG pipeline so the UI and API can be exercised without any external APIs or GPU. This is the default for CI and local development.

- **Why OpenAI instead of self-hosted models?**  
  The pipeline is implemented in a way that the retriever and generator are pluggable, but OpenAI embeddings + GPT are used as a pragmatic baseline for a personal portfolio project.

- **Why "answer only from retrieved context"?**  
  The generation step is constrained to the retrieved snippets and uses explicit instructions to say "I do not have enough information" when recall is low, trading off creativity for reliability.

- **Why keep CLIP / heavier multimodal components optional?**  
  Multimodal and heavy Torch dependencies are kept out of the Streamlit-only requirements so that lightweight deployments (e.g., Streamlit Cloud) do not have to build GPU stacks.

- **Why FAISS vs. hosted vector DBs?**  
  For local and educational usage, an in-process index (FAISS) is easier to reproduce and reason about. The code is structured so that a hosted vector database (like Pinecone) can be swapped in for production if needed.

## ğŸ¤ Contributing

Contributions welcome! Please:
1. Fork the repository
2. Create a feature branch
3. Add tests for new features
4. Submit a pull request

## ğŸ“ License

MIT License - see [LICENSE](LICENSE) file for details.

## ğŸ™ Acknowledgments

- OpenAI for GPT models and embeddings
- FastAPI and Streamlit teams
- RAG research community

## ğŸ“§ Contact

For questions or feedback, please open an issue on GitHub.

---

**Built with â¤ï¸ for demonstrating production-ready RAG systems with anti-hallucination strategies**
