{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 03 - RAG Pipeline Implementation\n",
    "\n",
    "This notebook covers:\n",
    "- Building a complete RAG pipeline with LangChain\n",
    "- Integrating vector search with LLM reasoning\n",
    "- Implementing multimodal query understanding\n",
    "- Creating custom retrieval chains\n",
    "- Prompt engineering for better responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.append('../src')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import faiss\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv\n",
    "from typing import List, Dict, Any\n",
    "\n",
    "# LangChain imports\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "from langchain.schema.output_parser import StrOutputParser\n",
    "from langchain.schema import Document\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Paths\n",
    "PROCESSED_DIR = Path('../data/processed')\n",
    "INDEX_DIR = Path('../data/processed/indexes')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Vector Database and Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load metadata\n",
    "metadata_df = pd.read_csv(PROCESSED_DIR / 'metadata_processed.csv')\n",
    "print(f\"Loaded {len(metadata_df)} items\")\n",
    "\n",
    "# Load embeddings\n",
    "text_embeddings = np.load(PROCESSED_DIR / 'text_embeddings.npy')\n",
    "clip_embeddings = np.load(PROCESSED_DIR / 'clip_text_embeddings.npy')\n",
    "\n",
    "print(f\"Text embeddings shape: {text_embeddings.shape}\")\n",
    "print(f\"CLIP embeddings shape: {clip_embeddings.shape}\")\n",
    "\n",
    "metadata_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Custom Retriever Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultimodalRetriever:\n",
    "    \"\"\"\n",
    "    Custom retriever that supports both text and image queries\n",
    "    \"\"\"\n",
    "    def __init__(self, index_path, embeddings, metadata_df, embedding_model=None):\n",
    "        self.index = faiss.read_index(str(index_path))\n",
    "        self.embeddings = embeddings\n",
    "        self.metadata_df = metadata_df\n",
    "        self.embedding_model = embedding_model or OpenAIEmbeddings(model=\"text-embedding-3-large\")\n",
    "    \n",
    "    def retrieve(self, query: str, k: int = 5, use_rerank: bool = False) -> List[Dict[str, Any]]:\n",
    "        \"\"\"\n",
    "        Retrieve top-k relevant documents for a text query\n",
    "        \"\"\"\n",
    "        # Generate query embedding\n",
    "        query_embedding = self.embedding_model.embed_query(query)\n",
    "        query_vector = np.array(query_embedding, dtype='float32')\n",
    "        \n",
    "        # Normalize for cosine similarity\n",
    "        query_vector = query_vector / np.linalg.norm(query_vector)\n",
    "        query_vector = query_vector.reshape(1, -1)\n",
    "        \n",
    "        # Search\n",
    "        distances, indices = self.index.search(query_vector, k)\n",
    "        \n",
    "        # Prepare results\n",
    "        results = []\n",
    "        for idx, dist in zip(indices[0], distances[0]):\n",
    "            if idx < len(self.metadata_df):\n",
    "                item = self.metadata_df.iloc[idx]\n",
    "                results.append({\n",
    "                    'id': item['id'],\n",
    "                    'title': item['title'],\n",
    "                    'description': item['description'],\n",
    "                    'category': item['category'],\n",
    "                    'image_path': item['image_path'],\n",
    "                    'score': float(1 - dist),  # Convert distance to similarity\n",
    "                    'content': f\"{item['title']}. {item['description']}\"\n",
    "                })\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def retrieve_with_image(self, image_embedding: np.ndarray, k: int = 5) -> List[Dict[str, Any]]:\n",
    "        \"\"\"\n",
    "        Retrieve top-k relevant documents for an image query\n",
    "        \"\"\"\n",
    "        # Normalize and search\n",
    "        query_vector = image_embedding / np.linalg.norm(image_embedding)\n",
    "        query_vector = query_vector.reshape(1, -1).astype('float32')\n",
    "        \n",
    "        distances, indices = self.index.search(query_vector, k)\n",
    "        \n",
    "        # Prepare results (same as retrieve)\n",
    "        results = []\n",
    "        for idx, dist in zip(indices[0], distances[0]):\n",
    "            if idx < len(self.metadata_df):\n",
    "                item = self.metadata_df.iloc[idx]\n",
    "                results.append({\n",
    "                    'id': item['id'],\n",
    "                    'title': item['title'],\n",
    "                    'description': item['description'],\n",
    "                    'category': item['category'],\n",
    "                    'image_path': item['image_path'],\n",
    "                    'score': float(1 - dist),\n",
    "                    'content': f\"{item['title']}. {item['description']}\"\n",
    "                })\n",
    "        \n",
    "        return results\n",
    "\n",
    "# Initialize retriever\n",
    "retriever = MultimodalRetriever(\n",
    "    index_path=INDEX_DIR / 'faiss_text.index',\n",
    "    embeddings=text_embeddings,\n",
    "    metadata_df=metadata_df\n",
    ")\n",
    "\n",
    "print(\"Retriever initialized successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Test Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test retrieval\n",
    "test_query = \"comfortable clothing for everyday wear\"\n",
    "results = retriever.retrieve(test_query, k=3)\n",
    "\n",
    "print(f\"Query: {test_query}\\n\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for i, result in enumerate(results):\n",
    "    print(f\"\\nRank {i+1} | Score: {result['score']:.4f}\")\n",
    "    print(f\"Title: {result['title']}\")\n",
    "    print(f\"Description: {result['description']}\")\n",
    "    print(f\"Category: {result['category']}\")\n",
    "    print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. RAG Pipeline with LangChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize LLM\n",
    "llm = ChatOpenAI(\n",
    "    model=\"gpt-4-turbo-preview\",\n",
    "    temperature=0.7,\n",
    "    api_key=os.getenv('OPENAI_API_KEY')\n",
    ")\n",
    "\n",
    "# Create RAG prompt template\n",
    "rag_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"\"\"You are a helpful shopping assistant with expertise in product recommendations.\n",
    "    Use the following retrieved product information to answer the user's question.\n",
    "    \n",
    "    If the question cannot be answered using the provided context, say so politely and suggest what information might help.\n",
    "    \n",
    "    Be concise, friendly, and focus on the most relevant products.\n",
    "    \n",
    "    Retrieved Products:\n",
    "    {context}\n",
    "    \"\"\"),\n",
    "    (\"human\", \"{question}\")\n",
    "])\n",
    "\n",
    "print(\"LLM and prompt template initialized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_context(results: List[Dict[str, Any]]) -> str:\n",
    "    \"\"\"\n",
    "    Format retrieval results into context string for LLM\n",
    "    \"\"\"\n",
    "    context_parts = []\n",
    "    for i, result in enumerate(results):\n",
    "        context_parts.append(\n",
    "            f\"Product {i+1}:\\n\"\n",
    "            f\"- Title: {result['title']}\\n\"\n",
    "            f\"- Description: {result['description']}\\n\"\n",
    "            f\"- Category: {result['category']}\\n\"\n",
    "            f\"- Relevance Score: {result['score']:.2f}\\n\"\n",
    "        )\n",
    "    return \"\\n\".join(context_parts)\n",
    "\n",
    "def rag_query(question: str, k: int = 5) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Execute complete RAG pipeline: retrieve + generate\n",
    "    \"\"\"\n",
    "    # Step 1: Retrieve relevant documents\n",
    "    results = retriever.retrieve(question, k=k)\n",
    "    \n",
    "    # Step 2: Format context\n",
    "    context = format_context(results)\n",
    "    \n",
    "    # Step 3: Generate response with LLM\n",
    "    chain = rag_prompt | llm | StrOutputParser()\n",
    "    \n",
    "    response = chain.invoke({\n",
    "        \"context\": context,\n",
    "        \"question\": question\n",
    "    })\n",
    "    \n",
    "    return {\n",
    "        \"question\": question,\n",
    "        \"answer\": response,\n",
    "        \"retrieved_products\": results,\n",
    "        \"context\": context\n",
    "    }\n",
    "\n",
    "print(\"RAG pipeline functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Test RAG Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test query 1: General product search\n",
    "question1 = \"What comfortable clothing items do you have for casual wear?\"\n",
    "\n",
    "result1 = rag_query(question1, k=3)\n",
    "\n",
    "print(\"Question:\", result1['question'])\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"\\nAI Assistant Response:\")\n",
    "print(result1['answer'])\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"\\nRetrieved Products:\")\n",
    "for i, prod in enumerate(result1['retrieved_products']):\n",
    "    print(f\"\\n{i+1}. {prod['title']} (Score: {prod['score']:.3f})\")\n",
    "    print(f\"   {prod['description']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test query 2: Specific product recommendation\n",
    "question2 = \"I'm looking for something to wear with blue jeans. What would you recommend?\"\n",
    "\n",
    "result2 = rag_query(question2, k=3)\n",
    "\n",
    "print(\"Question:\", result2['question'])\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"\\nAI Assistant Response:\")\n",
    "print(result2['answer'])\n",
    "print(\"\\n\" + \"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Advanced: Multi-Query RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multi-query generation prompt\n",
    "multi_query_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"\"\"You are an AI assistant that helps generate multiple search queries.\n",
    "    Given a user question, generate 3 different search queries that could help find relevant products.\n",
    "    Each query should approach the question from a different angle.\n",
    "    \n",
    "    Return ONLY the queries, one per line, without numbering or explanation.\"\"\"),\n",
    "    (\"human\", \"{question}\")\n",
    "])\n",
    "\n",
    "def multi_query_rag(question: str, k: int = 3) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Advanced RAG with multi-query expansion\n",
    "    \"\"\"\n",
    "    # Step 1: Generate multiple queries\n",
    "    query_chain = multi_query_prompt | llm | StrOutputParser()\n",
    "    generated_queries = query_chain.invoke({\"question\": question})\n",
    "    queries = [q.strip() for q in generated_queries.split('\\n') if q.strip()]\n",
    "    queries = [question] + queries[:3]  # Include original + top 3 generated\n",
    "    \n",
    "    print(f\"Generated {len(queries)} queries:\")\n",
    "    for i, q in enumerate(queries):\n",
    "        print(f\"  {i+1}. {q}\")\n",
    "    \n",
    "    # Step 2: Retrieve for each query and combine results\n",
    "    all_results = {}\n",
    "    for query in queries:\n",
    "        results = retriever.retrieve(query, k=k)\n",
    "        for result in results:\n",
    "            doc_id = result['id']\n",
    "            if doc_id not in all_results:\n",
    "                all_results[doc_id] = result\n",
    "            else:\n",
    "                # Boost score if retrieved multiple times\n",
    "                all_results[doc_id]['score'] = max(all_results[doc_id]['score'], result['score'])\n",
    "    \n",
    "    # Step 3: Sort by score and take top-k\n",
    "    combined_results = sorted(all_results.values(), key=lambda x: x['score'], reverse=True)[:k]\n",
    "    \n",
    "    # Step 4: Generate final response\n",
    "    context = format_context(combined_results)\n",
    "    chain = rag_prompt | llm | StrOutputParser()\n",
    "    \n",
    "    response = chain.invoke({\n",
    "        \"context\": context,\n",
    "        \"question\": question\n",
    "    })\n",
    "    \n",
    "    return {\n",
    "        \"question\": question,\n",
    "        \"generated_queries\": queries,\n",
    "        \"answer\": response,\n",
    "        \"retrieved_products\": combined_results\n",
    "    }\n",
    "\n",
    "print(\"Multi-query RAG pipeline defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test multi-query RAG\n",
    "question3 = \"I need outfit suggestions for a casual weekend\"\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"MULTI-QUERY RAG TEST\")\n",
    "print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "result3 = multi_query_rag(question3, k=5)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"AI Assistant Response:\")\n",
    "print(\"=\"*80)\n",
    "print(result3['answer'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Conversational RAG with Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "\n",
    "class ConversationalRAG:\n",
    "    \"\"\"\n",
    "    RAG system with conversation history\n",
    "    \"\"\"\n",
    "    def __init__(self, retriever, llm):\n",
    "        self.retriever = retriever\n",
    "        self.llm = llm\n",
    "        self.conversation_history = []\n",
    "        \n",
    "        self.prompt = ChatPromptTemplate.from_messages([\n",
    "            (\"system\", \"\"\"You are a helpful shopping assistant. Use the conversation history and retrieved products to provide personalized recommendations.\n",
    "            \n",
    "            Conversation History:\n",
    "            {history}\n",
    "            \n",
    "            Retrieved Products:\n",
    "            {context}\n",
    "            \"\"\"),\n",
    "            (\"human\", \"{question}\")\n",
    "        ])\n",
    "    \n",
    "    def chat(self, question: str, k: int = 3) -> str:\n",
    "        \"\"\"Process a conversational query\"\"\"\n",
    "        # Retrieve relevant products\n",
    "        results = self.retriever.retrieve(question, k=k)\n",
    "        context = format_context(results)\n",
    "        \n",
    "        # Format conversation history\n",
    "        history = \"\\n\".join([\n",
    "            f\"Human: {h['question']}\\nAssistant: {h['answer']}\"\n",
    "            for h in self.conversation_history[-3:]  # Last 3 exchanges\n",
    "        ])\n",
    "        \n",
    "        # Generate response\n",
    "        chain = self.prompt | self.llm | StrOutputParser()\n",
    "        response = chain.invoke({\n",
    "            \"history\": history,\n",
    "            \"context\": context,\n",
    "            \"question\": question\n",
    "        })\n",
    "        \n",
    "        # Update conversation history\n",
    "        self.conversation_history.append({\n",
    "            \"question\": question,\n",
    "            \"answer\": response,\n",
    "            \"retrieved_products\": results\n",
    "        })\n",
    "        \n",
    "        return response\n",
    "    \n",
    "    def reset(self):\n",
    "        \"\"\"Clear conversation history\"\"\"\n",
    "        self.conversation_history = []\n",
    "\n",
    "# Initialize conversational RAG\n",
    "conv_rag = ConversationalRAG(retriever, llm)\n",
    "\n",
    "print(\"Conversational RAG initialized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test conversational flow\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"CONVERSATIONAL RAG TEST\")\n",
    "print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "# Turn 1\n",
    "response1 = conv_rag.chat(\"Show me some casual clothing\")\n",
    "print(\"User: Show me some casual clothing\")\n",
    "print(f\"\\nAssistant: {response1}\")\n",
    "print(\"\\n\" + \"-\"*80 + \"\\n\")\n",
    "\n",
    "# Turn 2\n",
    "response2 = conv_rag.chat(\"What would go well with the first item?\")\n",
    "print(\"User: What would go well with the first item?\")\n",
    "print(f\"\\nAssistant: {response2}\")\n",
    "print(\"\\n\" + \"-\"*80 + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Save RAG Pipeline for API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Save RAG components\n",
    "rag_components = {\n",
    "    'retriever': retriever,\n",
    "    'llm': llm,\n",
    "    'rag_prompt': rag_prompt,\n",
    "    'multi_query_prompt': multi_query_prompt\n",
    "}\n",
    "\n",
    "with open(PROCESSED_DIR / 'rag_pipeline.pkl', 'wb') as f:\n",
    "    pickle.dump(rag_components, f)\n",
    "\n",
    "print(\"RAG pipeline components saved successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this notebook, we:\n",
    "1. Built a custom multimodal retriever supporting text and image queries\n",
    "2. Created a complete RAG pipeline with LangChain\n",
    "3. Implemented prompt engineering for better responses\n",
    "4. Developed multi-query expansion for improved retrieval\n",
    "5. Added conversational capabilities with memory\n",
    "6. Saved pipeline components for API integration\n",
    "\n",
    "Next step: Notebook 04 - Evaluation and Testing"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
