{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 01 - Data Processing and Embedding Generation\n",
    "\n",
    "This notebook covers:\n",
    "- Loading and preprocessing text and image data\n",
    "- Generating embeddings using CLIP and OpenAI models\n",
    "- Saving processed embeddings for indexing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "# !pip install -r ../requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.append('../src')\n",
    "\n",
    "import torch\n",
    "import clip\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Configuration\n",
    "DATA_DIR = Path('../data/raw')\n",
    "PROCESSED_DIR = Path('../data/processed')\n",
    "PROCESSED_DIR.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load CLIP Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load CLIP model\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "clip_model, preprocess = clip.load(\"ViT-B/32\", device=device)\n",
    "print(\"CLIP model loaded successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Sample Dataset\n",
    "\n",
    "Example structure:\n",
    "```\n",
    "data/raw/\n",
    "├── images/\n",
    "│   ├── product_001.jpg\n",
    "│   ├── product_002.jpg\n",
    "│   └── ...\n",
    "└── metadata.csv  # Contains: id, title, description, image_path, category\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load metadata\n",
    "metadata_path = DATA_DIR / 'metadata.csv'\n",
    "\n",
    "# Create sample data if not exists\n",
    "if not metadata_path.exists():\n",
    "    print(\"Creating sample metadata...\")\n",
    "    sample_data = {\n",
    "        'id': ['prod_001', 'prod_002', 'prod_003'],\n",
    "        'title': ['Red T-Shirt', 'Blue Jeans', 'White Sneakers'],\n",
    "        'description': [\n",
    "            'Comfortable red cotton t-shirt with round neck',\n",
    "            'Classic blue denim jeans with regular fit',\n",
    "            'White canvas sneakers with rubber sole'\n",
    "        ],\n",
    "        'image_path': ['images/prod_001.jpg', 'images/prod_002.jpg', 'images/prod_003.jpg'],\n",
    "        'category': ['clothing', 'clothing', 'footwear']\n",
    "    }\n",
    "    df = pd.DataFrame(sample_data)\n",
    "    df.to_csv(metadata_path, index=False)\n",
    "else:\n",
    "    df = pd.read_csv(metadata_path)\n",
    "\n",
    "print(f\"Loaded {len(df)} items\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Generate Text Embeddings (OpenAI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize OpenAI client\n",
    "client = OpenAI(api_key=os.getenv('OPENAI_API_KEY'))\n",
    "\n",
    "def get_text_embedding(text, model=\"text-embedding-3-large\"):\n",
    "    \"\"\"Generate embedding for text using OpenAI API\"\"\"\n",
    "    try:\n",
    "        response = client.embeddings.create(\n",
    "            input=text,\n",
    "            model=model\n",
    "        )\n",
    "        return response.data[0].embedding\n",
    "    except Exception as e:\n",
    "        print(f\"Error generating embedding: {e}\")\n",
    "        return None\n",
    "\n",
    "# Generate embeddings for all text descriptions\n",
    "print(\"Generating text embeddings...\")\n",
    "text_embeddings = []\n",
    "\n",
    "for idx, row in tqdm(df.iterrows(), total=len(df)):\n",
    "    # Combine title and description\n",
    "    combined_text = f\"{row['title']}. {row['description']}\"\n",
    "    embedding = get_text_embedding(combined_text)\n",
    "    text_embeddings.append(embedding)\n",
    "\n",
    "df['text_embedding'] = text_embeddings\n",
    "print(f\"Generated {len(text_embeddings)} text embeddings\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Generate Image Embeddings (CLIP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_image_embedding(image_path):\n",
    "    \"\"\"Generate embedding for image using CLIP\"\"\"\n",
    "    try:\n",
    "        image = Image.open(image_path).convert('RGB')\n",
    "        image_input = preprocess(image).unsqueeze(0).to(device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            image_features = clip_model.encode_image(image_input)\n",
    "            image_features /= image_features.norm(dim=-1, keepdim=True)\n",
    "        \n",
    "        return image_features.cpu().numpy()[0].tolist()\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing image {image_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "# Generate image embeddings\n",
    "print(\"Generating image embeddings...\")\n",
    "image_embeddings = []\n",
    "\n",
    "for idx, row in tqdm(df.iterrows(), total=len(df)):\n",
    "    image_path = DATA_DIR / row['image_path']\n",
    "    if image_path.exists():\n",
    "        embedding = get_image_embedding(image_path)\n",
    "        image_embeddings.append(embedding)\n",
    "    else:\n",
    "        print(f\"Image not found: {image_path}\")\n",
    "        image_embeddings.append(None)\n",
    "\n",
    "df['image_embedding'] = image_embeddings\n",
    "print(f\"Generated {len([e for e in image_embeddings if e is not None])} image embeddings\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Generate CLIP Text Embeddings (for multimodal alignment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_clip_text_embedding(text):\n",
    "    \"\"\"Generate text embedding using CLIP (aligned with image space)\"\"\"\n",
    "    try:\n",
    "        text_input = clip.tokenize([text]).to(device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            text_features = clip_model.encode_text(text_input)\n",
    "            text_features /= text_features.norm(dim=-1, keepdim=True)\n",
    "        \n",
    "        return text_features.cpu().numpy()[0].tolist()\n",
    "    except Exception as e:\n",
    "        print(f\"Error generating CLIP text embedding: {e}\")\n",
    "        return None\n",
    "\n",
    "# Generate CLIP text embeddings\n",
    "print(\"Generating CLIP text embeddings...\")\n",
    "clip_text_embeddings = []\n",
    "\n",
    "for idx, row in tqdm(df.iterrows(), total=len(df)):\n",
    "    combined_text = f\"{row['title']}. {row['description']}\"\n",
    "    embedding = get_clip_text_embedding(combined_text)\n",
    "    clip_text_embeddings.append(embedding)\n",
    "\n",
    "df['clip_text_embedding'] = clip_text_embeddings\n",
    "print(f\"Generated {len(clip_text_embeddings)} CLIP text embeddings\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Save Processed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save embeddings as separate numpy files for efficiency\n",
    "import pickle\n",
    "\n",
    "# Save text embeddings\n",
    "text_emb_array = np.array([e for e in df['text_embedding'] if e is not None])\n",
    "np.save(PROCESSED_DIR / 'text_embeddings.npy', text_emb_array)\n",
    "\n",
    "# Save image embeddings\n",
    "image_emb_array = np.array([e for e in df['image_embedding'] if e is not None])\n",
    "np.save(PROCESSED_DIR / 'image_embeddings.npy', image_emb_array)\n",
    "\n",
    "# Save CLIP text embeddings\n",
    "clip_text_emb_array = np.array([e for e in df['clip_text_embedding'] if e is not None])\n",
    "np.save(PROCESSED_DIR / 'clip_text_embeddings.npy', clip_text_emb_array)\n",
    "\n",
    "# Save metadata (without embeddings for readability)\n",
    "df_metadata = df[['id', 'title', 'description', 'image_path', 'category']].copy()\n",
    "df_metadata.to_csv(PROCESSED_DIR / 'metadata_processed.csv', index=False)\n",
    "\n",
    "# Save full dataframe with embeddings\n",
    "with open(PROCESSED_DIR / 'full_data.pkl', 'wb') as f:\n",
    "    pickle.dump(df, f)\n",
    "\n",
    "print(\"\\nData saved successfully!\")\n",
    "print(f\"Text embeddings shape: {text_emb_array.shape}\")\n",
    "print(f\"Image embeddings shape: {image_emb_array.shape}\")\n",
    "print(f\"CLIP text embeddings shape: {clip_text_emb_array.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Visualization (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Visualize embeddings using t-SNE\n",
    "def visualize_embeddings(embeddings, labels, title):\n",
    "    if len(embeddings) < 2:\n",
    "        print(\"Not enough samples for visualization\")\n",
    "        return\n",
    "    \n",
    "    tsne = TSNE(n_components=2, random_state=42)\n",
    "    embeddings_2d = tsne.fit_transform(embeddings)\n",
    "    \n",
    "    plt.figure(figsize=(10, 8))\n",
    "    scatter = plt.scatter(embeddings_2d[:, 0], embeddings_2d[:, 1], \n",
    "                         c=range(len(labels)), cmap='viridis', s=100)\n",
    "    \n",
    "    for i, label in enumerate(labels):\n",
    "        plt.annotate(label, (embeddings_2d[i, 0], embeddings_2d[i, 1]), \n",
    "                    fontsize=8, alpha=0.7)\n",
    "    \n",
    "    plt.title(title)\n",
    "    plt.xlabel('t-SNE Dimension 1')\n",
    "    plt.ylabel('t-SNE Dimension 2')\n",
    "    plt.colorbar(scatter)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Visualize CLIP text embeddings\n",
    "if len(clip_text_emb_array) > 0:\n",
    "    visualize_embeddings(\n",
    "        clip_text_emb_array,\n",
    "        df_metadata['title'].tolist(),\n",
    "        'CLIP Text Embeddings (t-SNE)'\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this notebook, we:\n",
    "1. Loaded and preprocessed multimodal data\n",
    "2. Generated text embeddings using OpenAI's text-embedding-3-large\n",
    "3. Generated image embeddings using CLIP\n",
    "4. Generated CLIP text embeddings for multimodal alignment\n",
    "5. Saved all processed embeddings for vector database indexing\n",
    "\n",
    "Next step: Notebook 02 - Vector Database Setup and Indexing"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
