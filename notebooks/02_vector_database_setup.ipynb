{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 02 - Vector Database Setup and Indexing\n",
    "\n",
    "This notebook covers:\n",
    "- Setting up FAISS, Pinecone, and ChromaDB vector databases\n",
    "- Indexing embeddings from the previous notebook\n",
    "- Implementing similarity search\n",
    "- Performance comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.append('../src')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv\n",
    "import time\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Paths\n",
    "PROCESSED_DIR = Path('../data/processed')\n",
    "INDEX_DIR = Path('../data/processed/indexes')\n",
    "INDEX_DIR.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Processed Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load embeddings\n",
    "text_embeddings = np.load(PROCESSED_DIR / 'text_embeddings.npy')\n",
    "image_embeddings = np.load(PROCESSED_DIR / 'image_embeddings.npy')\n",
    "clip_text_embeddings = np.load(PROCESSED_DIR / 'clip_text_embeddings.npy')\n",
    "\n",
    "# Load metadata\n",
    "metadata_df = pd.read_csv(PROCESSED_DIR / 'metadata_processed.csv')\n",
    "\n",
    "print(f\"Loaded {len(text_embeddings)} text embeddings of dimension {text_embeddings.shape[1]}\")\n",
    "print(f\"Loaded {len(image_embeddings)} image embeddings of dimension {image_embeddings.shape[1]}\")\n",
    "print(f\"Loaded {len(clip_text_embeddings)} CLIP text embeddings of dimension {clip_text_embeddings.shape[1]}\")\n",
    "print(f\"\\nMetadata: {len(metadata_df)} items\")\n",
    "metadata_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. FAISS Vector Database Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import faiss\n",
    "\n",
    "class FAISSIndex:\n",
    "    def __init__(self, dimension, index_type='flatl2'):\n",
    "        \"\"\"\n",
    "        Initialize FAISS index\n",
    "        index_type: 'flatl2', 'flatip' (inner product), 'ivf' (inverted file)\n",
    "        \"\"\"\n",
    "        self.dimension = dimension\n",
    "        \n",
    "        if index_type == 'flatl2':\n",
    "            self.index = faiss.IndexFlatL2(dimension)\n",
    "        elif index_type == 'flatip':\n",
    "            self.index = faiss.IndexFlatIP(dimension)\n",
    "        elif index_type == 'ivf':\n",
    "            # IVF with 100 clusters for faster search on large datasets\n",
    "            quantizer = faiss.IndexFlatL2(dimension)\n",
    "            self.index = faiss.IndexIVFFlat(quantizer, dimension, 100)\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown index type: {index_type}\")\n",
    "    \n",
    "    def add_vectors(self, vectors):\n",
    "        \"\"\"Add vectors to the index\"\"\"\n",
    "        vectors = vectors.astype('float32')\n",
    "        \n",
    "        # Train IVF index if needed\n",
    "        if isinstance(self.index, faiss.IndexIVFFlat):\n",
    "            if not self.index.is_trained:\n",
    "                self.index.train(vectors)\n",
    "        \n",
    "        self.index.add(vectors)\n",
    "        print(f\"Added {len(vectors)} vectors. Total: {self.index.ntotal}\")\n",
    "    \n",
    "    def search(self, query_vector, k=5):\n",
    "        \"\"\"Search for k nearest neighbors\"\"\"\n",
    "        query_vector = query_vector.astype('float32').reshape(1, -1)\n",
    "        distances, indices = self.index.search(query_vector, k)\n",
    "        return distances[0], indices[0]\n",
    "    \n",
    "    def save(self, path):\n",
    "        \"\"\"Save index to disk\"\"\"\n",
    "        faiss.write_index(self.index, str(path))\n",
    "        print(f\"Index saved to {path}\")\n",
    "    \n",
    "    @classmethod\n",
    "    def load(cls, path):\n",
    "        \"\"\"Load index from disk\"\"\"\n",
    "        obj = cls.__new__(cls)\n",
    "        obj.index = faiss.read_index(str(path))\n",
    "        obj.dimension = obj.index.d\n",
    "        print(f\"Index loaded from {path}\")\n",
    "        return obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create FAISS indexes for different embedding types\n",
    "print(\"Creating FAISS indexes...\\n\")\n",
    "\n",
    "# Text embeddings index (cosine similarity via inner product)\n",
    "text_index = FAISSIndex(text_embeddings.shape[1], index_type='flatip')\n",
    "# Normalize for cosine similarity\n",
    "text_embeddings_norm = text_embeddings / np.linalg.norm(text_embeddings, axis=1, keepdims=True)\n",
    "text_index.add_vectors(text_embeddings_norm)\n",
    "\n",
    "# Image embeddings index\n",
    "image_index = FAISSIndex(image_embeddings.shape[1], index_type='flatip')\n",
    "image_embeddings_norm = image_embeddings / np.linalg.norm(image_embeddings, axis=1, keepdims=True)\n",
    "image_index.add_vectors(image_embeddings_norm)\n",
    "\n",
    "# CLIP text embeddings index (multimodal)\n",
    "clip_index = FAISSIndex(clip_text_embeddings.shape[1], index_type='flatip')\n",
    "clip_embeddings_norm = clip_text_embeddings / np.linalg.norm(clip_text_embeddings, axis=1, keepdims=True)\n",
    "clip_index.add_vectors(clip_embeddings_norm)\n",
    "\n",
    "# Save indexes\n",
    "text_index.save(INDEX_DIR / 'faiss_text.index')\n",
    "image_index.save(INDEX_DIR / 'faiss_image.index')\n",
    "clip_index.save(INDEX_DIR / 'faiss_clip.index')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. ChromaDB Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import chromadb\n",
    "from chromadb.config import Settings\n",
    "\n",
    "# Initialize ChromaDB client\n",
    "chroma_client = chromadb.PersistentClient(\n",
    "    path=str(INDEX_DIR / 'chromadb'),\n",
    "    settings=Settings(anonymized_telemetry=False)\n",
    ")\n",
    "\n",
    "# Create collections for different modalities\n",
    "print(\"Creating ChromaDB collections...\\n\")\n",
    "\n",
    "# Delete existing collections if they exist\n",
    "try:\n",
    "    chroma_client.delete_collection(\"text_embeddings\")\n",
    "    chroma_client.delete_collection(\"image_embeddings\")\n",
    "    chroma_client.delete_collection(\"multimodal_embeddings\")\n",
    "except:\n",
    "    pass\n",
    "\n",
    "# Text collection\n",
    "text_collection = chroma_client.create_collection(\n",
    "    name=\"text_embeddings\",\n",
    "    metadata={\"description\": \"OpenAI text embeddings\"}\n",
    ")\n",
    "\n",
    "# Image collection\n",
    "image_collection = chroma_client.create_collection(\n",
    "    name=\"image_embeddings\",\n",
    "    metadata={\"description\": \"CLIP image embeddings\"}\n",
    ")\n",
    "\n",
    "# Multimodal collection (CLIP text + image aligned)\n",
    "multimodal_collection = chroma_client.create_collection(\n",
    "    name=\"multimodal_embeddings\",\n",
    "    metadata={\"description\": \"CLIP multimodal embeddings\"}\n",
    ")\n",
    "\n",
    "print(\"Collections created successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add documents to ChromaDB collections\n",
    "print(\"Adding documents to ChromaDB...\\n\")\n",
    "\n",
    "# Prepare documents and metadata\n",
    "ids = metadata_df['id'].tolist()\n",
    "documents = [f\"{row['title']}. {row['description']}\" for _, row in metadata_df.iterrows()]\n",
    "metadatas = metadata_df[['title', 'category', 'image_path']].to_dict('records')\n",
    "\n",
    "# Add to text collection\n",
    "text_collection.add(\n",
    "    ids=ids,\n",
    "    embeddings=text_embeddings.tolist(),\n",
    "    documents=documents,\n",
    "    metadatas=metadatas\n",
    ")\n",
    "print(f\"Added {len(ids)} items to text collection\")\n",
    "\n",
    "# Add to image collection\n",
    "image_collection.add(\n",
    "    ids=ids,\n",
    "    embeddings=image_embeddings.tolist(),\n",
    "    documents=documents,\n",
    "    metadatas=metadatas\n",
    ")\n",
    "print(f\"Added {len(ids)} items to image collection\")\n",
    "\n",
    "# Add to multimodal collection\n",
    "multimodal_collection.add(\n",
    "    ids=ids,\n",
    "    embeddings=clip_text_embeddings.tolist(),\n",
    "    documents=documents,\n",
    "    metadatas=metadatas\n",
    ")\n",
    "print(f\"Added {len(ids)} items to multimodal collection\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Pinecone Setup (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment to use Pinecone\n",
    "\n",
    "# from pinecone import Pinecone, ServerlessSpec\n",
    "\n",
    "# # Initialize Pinecone\n",
    "# pc = Pinecone(api_key=os.getenv('PINECONE_API_KEY'))\n",
    "\n",
    "# # Create index\n",
    "# index_name = \"multimodal-rag\"\n",
    "\n",
    "# # Delete existing index if it exists\n",
    "# if index_name in pc.list_indexes().names():\n",
    "#     pc.delete_index(index_name)\n",
    "\n",
    "# # Create new index\n",
    "# pc.create_index(\n",
    "#     name=index_name,\n",
    "#     dimension=text_embeddings.shape[1],\n",
    "#     metric='cosine',\n",
    "#     spec=ServerlessSpec(\n",
    "#         cloud='aws',\n",
    "#         region='us-east-1'\n",
    "#     )\n",
    "# )\n",
    "\n",
    "# # Connect to index\n",
    "# index = pc.Index(index_name)\n",
    "\n",
    "# # Prepare vectors for upsert\n",
    "# vectors_to_upsert = []\n",
    "# for i, (idx, row) in enumerate(metadata_df.iterrows()):\n",
    "#     vectors_to_upsert.append({\n",
    "#         'id': row['id'],\n",
    "#         'values': text_embeddings[i].tolist(),\n",
    "#         'metadata': {\n",
    "#             'title': row['title'],\n",
    "#             'description': row['description'],\n",
    "#             'category': row['category']\n",
    "#         }\n",
    "#     })\n",
    "\n",
    "# # Upsert vectors\n",
    "# index.upsert(vectors=vectors_to_upsert)\n",
    "# print(f\"Upserted {len(vectors_to_upsert)} vectors to Pinecone\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Test Similarity Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_search_results(query, distances, indices, metadata_df, k=5):\n",
    "    \"\"\"Display search results in a readable format\"\"\"\n",
    "    print(f\"\\nQuery: {query}\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    for i, (dist, idx) in enumerate(zip(distances[:k], indices[:k])):\n",
    "        if idx < len(metadata_df):\n",
    "            item = metadata_df.iloc[idx]\n",
    "            print(f\"\\nRank {i+1} | Similarity: {1-dist:.4f}\")\n",
    "            print(f\"Title: {item['title']}\")\n",
    "            print(f\"Description: {item['description']}\")\n",
    "            print(f\"Category: {item['category']}\")\n",
    "            print(\"-\" * 80)\n",
    "\n",
    "# Test with FAISS\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"TESTING FAISS SEARCH\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Use first item as query\n",
    "test_query_idx = 0\n",
    "test_query_embedding = text_embeddings_norm[test_query_idx]\n",
    "test_query_text = f\"{metadata_df.iloc[test_query_idx]['title']}. {metadata_df.iloc[test_query_idx]['description']}\"\n",
    "\n",
    "# Search\n",
    "distances, indices = text_index.search(test_query_embedding, k=5)\n",
    "display_search_results(test_query_text, distances, indices, metadata_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with ChromaDB\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"TESTING CHROMADB SEARCH\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Query ChromaDB\n",
    "results = text_collection.query(\n",
    "    query_embeddings=[test_query_embedding.tolist()],\n",
    "    n_results=5\n",
    ")\n",
    "\n",
    "print(f\"\\nQuery: {test_query_text}\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for i, (doc, metadata, distance) in enumerate(zip(\n",
    "    results['documents'][0],\n",
    "    results['metadatas'][0],\n",
    "    results['distances'][0]\n",
    ")):\n",
    "    print(f\"\\nRank {i+1} | Distance: {distance:.4f}\")\n",
    "    print(f\"Title: {metadata['title']}\")\n",
    "    print(f\"Document: {doc}\")\n",
    "    print(f\"Category: {metadata['category']}\")\n",
    "    print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Performance Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def benchmark_search(index_func, query_embedding, n_queries=100, k=5):\n",
    "    \"\"\"Benchmark search performance\"\"\"\n",
    "    start_time = time.time()\n",
    "    \n",
    "    for _ in range(n_queries):\n",
    "        index_func(query_embedding, k)\n",
    "    \n",
    "    end_time = time.time()\n",
    "    avg_time = (end_time - start_time) / n_queries * 1000  # ms\n",
    "    \n",
    "    return avg_time\n",
    "\n",
    "print(\"\\nBenchmarking search performance...\\n\")\n",
    "\n",
    "# FAISS benchmark\n",
    "faiss_time = benchmark_search(text_index.search, test_query_embedding, n_queries=100)\n",
    "print(f\"FAISS average query time: {faiss_time:.3f} ms\")\n",
    "\n",
    "# ChromaDB benchmark\n",
    "def chroma_search(query_emb, k):\n",
    "    return text_collection.query(query_embeddings=[query_emb.tolist()], n_results=k)\n",
    "\n",
    "chroma_time = benchmark_search(chroma_search, test_query_embedding, n_queries=100)\n",
    "print(f\"ChromaDB average query time: {chroma_time:.3f} ms\")\n",
    "\n",
    "print(f\"\\nFAISS is {chroma_time/faiss_time:.2f}x faster than ChromaDB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Hybrid Search (Text + Image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hybrid_search(text_query_emb, image_query_emb, text_weight=0.5, k=5):\n",
    "    \"\"\"\n",
    "    Perform hybrid search combining text and image embeddings\n",
    "    \"\"\"\n",
    "    # Search in both indexes\n",
    "    text_distances, text_indices = text_index.search(text_query_emb, k=k*2)\n",
    "    image_distances, image_indices = image_index.search(image_query_emb, k=k*2)\n",
    "    \n",
    "    # Combine scores (convert distances to similarities)\n",
    "    text_scores = 1 - text_distances\n",
    "    image_scores = 1 - image_distances\n",
    "    \n",
    "    # Create a dictionary to aggregate scores\n",
    "    combined_scores = {}\n",
    "    \n",
    "    for idx, score in zip(text_indices, text_scores):\n",
    "        combined_scores[idx] = text_weight * score\n",
    "    \n",
    "    for idx, score in zip(image_indices, image_scores):\n",
    "        if idx in combined_scores:\n",
    "            combined_scores[idx] += (1 - text_weight) * score\n",
    "        else:\n",
    "            combined_scores[idx] = (1 - text_weight) * score\n",
    "    \n",
    "    # Sort by combined score\n",
    "    sorted_results = sorted(combined_scores.items(), key=lambda x: x[1], reverse=True)[:k]\n",
    "    \n",
    "    indices = np.array([idx for idx, _ in sorted_results])\n",
    "    scores = np.array([score for _, score in sorted_results])\n",
    "    \n",
    "    return scores, indices\n",
    "\n",
    "# Test hybrid search\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"TESTING HYBRID SEARCH (Text + Image)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "scores, indices = hybrid_search(\n",
    "    text_embeddings_norm[0],\n",
    "    image_embeddings_norm[0],\n",
    "    text_weight=0.6,\n",
    "    k=5\n",
    ")\n",
    "\n",
    "# Display results\n",
    "distances = 1 - scores  # Convert back to distances for display function\n",
    "display_search_results(\"Hybrid query (text + image)\", distances, indices, metadata_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this notebook, we:\n",
    "1. Set up FAISS vector database for fast similarity search\n",
    "2. Set up ChromaDB for managed vector storage\n",
    "3. Created indexes for text, image, and multimodal embeddings\n",
    "4. Implemented and tested similarity search\n",
    "5. Benchmarked performance across different vector databases\n",
    "6. Implemented hybrid search combining text and image modalities\n",
    "\n",
    "Next step: Notebook 03 - RAG Pipeline Implementation"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
